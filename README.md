# ğŸœï¸ Offroad Semantic Scene Segmentation

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)
![Conda](https://img.shields.io/badge/Conda-Environment-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**A semantic segmentation model trained on synthetic desert environments using Duality AI's digital twin platform**

[Report Bug](https://github.com/YOUR_USERNAME/YOUR_REPO/issues) Â· [Request Feature](https://github.com/YOUR_USERNAME/YOUR_REPO/issues) Â· [Duality AI Discord](https://discord.com/invite/dualityfalcommunity)

</div>

---

## ğŸ¯ Overview

This project is a submission for the **Duality AI Offroad Autonomy Segmentation Hackathon** â€” a challenge focused on training robust semantic segmentation models using synthetic data generated from Duality AI's **FalconCloud** geospatial digital twin platform.

The model is trained on annotated desert environment images and evaluated on a novel (unseen) desert scene, demonstrating how synthetic data can effectively bridge real-world data scarcity in off-road autonomy applications.

### Key Highlights

- **ğŸŒµ Synthetic Data Training**: Leverages Duality AI's Falcon digital twin platform for high-quality annotated desert scenes
- **ğŸ” Semantic Segmentation**: Pixel-level classification across 10 desert environment classes
- **ğŸ¤– DINOv2 Backbone**: Facebook's self-supervised ViT-S/14 pretrained on 142M images
- **ğŸ“Š IoU Evaluation**: Model performance benchmarked using Intersection over Union (IoU) score
- **âš¡ Optimized Inference**: Target inference speed under 50ms per image
- **ğŸ“ Comprehensive Documentation**: Full methodology, failure analysis, and reproducibility guide

---

## ğŸ‘¥ Team Kairo

| Name | Role | GitHub |
|------|------|--------|
| **Aditya Raj** | AI Engineering & Model Training | [@your-handle](https://github.com/your-handle) |
| **Harsh Pal** | AI Engineering & Infrastructure | [@your-handle](https://github.com/your-handle) |
| **Akshita Singh** | Documentation & Analysis | [@your-handle](https://github.com/your-handle) |
| **Fuzailur Rahman** | Documentation & Presentation | [@your-handle](https://github.com/your-handle) |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Image   â”‚
â”‚   (RGB Desert)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocessing      â”‚
â”‚  & Augmentation     â”‚
â”‚  448Ã—448 resize     â”‚
â”‚  ImageNet normalize â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DINOv2 ViT-S/14        â”‚â—„â”€â”€â”€â”€ Pretrained (142M images)
â”‚  Backbone               â”‚
â”‚  embed_dim = 384        â”‚
â”‚  patch_size = 14        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚  patch tokens (B, N, 384)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLP Decoder    â”‚
â”‚  384â†’256â†’128â†’10 â”‚
â”‚  + BatchNorm    â”‚
â”‚  + Dropout      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bilinear       â”‚
â”‚  Upsample       â”‚
â”‚  â†’ Full Res     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Segmentation   â”‚
â”‚  Mask Output    â”‚
â”‚  (10 Classes)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Features

### Core Functionality
- **ğŸ“„ Data Pipeline**: Automated loading and preprocessing for train/val/test splits
- **ğŸ¨ Visualization**: High-contrast color-coded segmentation output
- **ğŸ“ˆ Benchmarking**: IoU, Dice, and Accuracy tracking across all training epochs
- **ğŸ” Checkpointing**: Automatic saving of best model weights by validation IoU
- **ğŸ§ª Failure Analysis**: Side-by-side visualizations to identify misclassification cases

### Technical Features
- **Two-Phase Training**: Frozen backbone warmup â†’ full fine-tuning at epoch 5
- **Mixed Precision**: FP16 via `torch.cuda.amp` for faster training
- **Cosine LR Scheduling**: Smooth learning rate decay over all epochs
- **Modular Codebase**: Clean separation between training, evaluation, and visualization
- **Conda Environment**: Reproducible dependency management via the `EDU` environment
- **Cross-platform Setup**: Setup scripts for both Windows (`.bat`) and Mac/Linux (`.sh`)

---

## ğŸ› ï¸ Technology Stack

| Category | Technology |
|----------|------------|
| **Language** | Python 3.8+ |
| **Deep Learning** | PyTorch 2.0+ |
| **Backbone** | DINOv2 ViT-S/14 (Facebook Research) |
| **Environment** | Conda (EDU) |
| **Data Source** | Duality AI FalconCloud |
| **Visualization** | Matplotlib / Pillow |
| **Experiment Tracking** | Local logs + metric graphs |

---

## ğŸ·ï¸ Dataset Classes

All data is generated from Duality AI's FalconEditor across various desert environment digital twins.

| Class ID | Class Name     | Model Index | Description |
|----------|----------------|-------------|-------------|
| 100      | Trees          | 0 | Desert trees (e.g. Joshua trees) |
| 200      | Lush Bushes    | 1 | Dense, green shrubbery |
| 300      | Dry Grass      | 2 | Sparse dry grassland |
| 500      | Dry Bushes     | 3 | Dry desert shrubs |
| 550      | Ground Clutter | 4 | Small debris and mixed ground materials |
| 600      | Flowers        | 5 | Desert wildflowers |
| 700      | Logs           | 6 | Fallen logs and branches |
| 800      | Rocks          | 7 | Boulders and rocky terrain |
| 7100     | Landscape      | 8 | General ground (catch-all) |
| 10000    | Sky            | 9 | Sky pixels |

---

## ğŸ“ Project Structure

```
offroad-segmentation/
â”‚
â”œâ”€â”€ ENV_SETUP/
â”‚   â”œâ”€â”€ setup_env.bat          # Windows environment setup
â”‚   â””â”€â”€ setup_env.sh           # Mac/Linux environment setup
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ Train/
â”‚   â”‚   â”œâ”€â”€ rgb/               # Training RGB images
â”‚   â”‚   â””â”€â”€ seg/               # Training segmentation masks
â”‚   â”œâ”€â”€ Val/
â”‚   â”‚   â”œâ”€â”€ rgb/               # Validation RGB images
â”‚   â”‚   â””â”€â”€ seg/               # Validation segmentation masks
â”‚   â””â”€â”€ testImages/            # Unseen test images (DO NOT use for training)
â”‚
â”œâ”€â”€ train_stats/               # Auto-generated after training
â”‚   â”œâ”€â”€ training_curves.png    # Train vs Val loss
â”‚   â”œâ”€â”€ iou_curves.png         # Validation IoU per epoch
â”‚   â”œâ”€â”€ dice_curves.png        # Validation Dice per epoch
â”‚   â”œâ”€â”€ all_metrics_curves.png # Combined dashboard
â”‚   â””â”€â”€ evaluation_metrics.txt # Final numeric results
â”‚
â”œâ”€â”€ predictions/               # Auto-generated after testing
â”‚   â”œâ”€â”€ colorized/             # Color-coded segmentation masks
â”‚   â”œâ”€â”€ visualizations/        # Side-by-side comparison images
â”‚   â””â”€â”€ test_results.txt       # Per-image IoU breakdown
â”‚
â”œâ”€â”€ segmentation_head.pth      # Trained model weights (best checkpoint)
â”œâ”€â”€ train.py                   # Model training script
â”œâ”€â”€ test.py                    # Model evaluation & inference script
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### Prerequisites

- [Miniconda](https://docs.conda.io/en/latest/miniconda.html) or [Anaconda](https://www.anaconda.com/)
- A free [Falcon account](https://falcon.duality.ai/auth/sign-up?utm_source=hackathon&utm_medium=instructions&utm_campaign=GHR2)
- CUDA-capable GPU recommended (CPU training is supported but slow)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git
   cd YOUR_REPO_NAME
   ```

2. **Set up the Conda environment**

   **Windows (Anaconda Prompt):**
   ```bash
   cd ENV_SETUP
   setup_env.bat
   ```

   **Mac/Linux:**
   ```bash
   cd ENV_SETUP
   bash setup_env.sh
   ```
   > This creates a conda environment called `EDU` with all required dependencies.

3. **Download the dataset**

   Download from the [Duality AI Hackathon page](https://falcon.duality.ai/secure/documentation/hackathon-segmentation-desert?utm_source=hackathon&utm_medium=instructions&utm_campaign=GHR2) and place the contents into the `dataset/` folder following the structure above.
   > Navigate to the **Segmentation Track** section on the dataset page.

4. **Activate the environment**
   ```bash
   conda activate EDU
   ```

---

## ğŸ’¡ Usage

### Train the Model
```bash
python train.py
```
Trains the model using images from `dataset/Train/` and `dataset/Val/`. Best model saved to `segmentation_head.pth`. Graphs and metrics saved to `train_stats/`.

**Custom parameters:**
```bash
python train.py --epochs 30 --batch_size 4 --lr 0.0001 --img_size 448
```

### Evaluate on Test Images
```bash
python test.py
```
Runs inference on `dataset/testImages/` â€” images the model has **never seen** during training. Outputs colorized predictions and visualizations to `predictions/`.

**With ground truth IoU evaluation:**
```bash
python test.py --seg_dir dataset/Val/seg
```

---

## âš™ï¸ Configuration

Key hyperparameters in `train.py`:

```python
CONFIG = {
    "num_epochs":       30,    # Training epochs
    "batch_size":       4,     # Reduce to 2 if GPU runs out of memory
    "lr":               1e-4,  # Head learning rate (backbone uses lr * 0.1)
    "img_size":         448,   # Must be divisible by 14 (DINOv2 patch size)
    "unfreeze_epoch":   5,     # Epoch to unfreeze backbone for fine-tuning
}
```

---

## ğŸ“Š Results

### Baseline (10 Epochs, Frozen Backbone)

| Metric | Value |
|--------|-------|
| Val Loss | 0.8163 |
| **Val IoU** | **0.2921** |
| Val Dice | 0.4364 |
| Val Accuracy | 0.7024 |

### Optimized Run (30 Epochs, Full Fine-tuning)

| Metric | Value |
|--------|-------|
| Val Loss | _TBD_ |
| **Val IoU** | _TBD_ |
| Val Dice | _TBD_ |
| Val Accuracy | _TBD_ |

> ğŸ¯ **Benchmark targets:** Maximize Mean IoU Â· Inference speed < 50ms per image

---

## ğŸ§ª How It Works

### 1. Data Preparation
- RGB images and corresponding segmentation masks loaded from train/val splits
- Raw class pixel IDs (100, 200, ..., 10000) remapped to contiguous indices 0â€“9
- Augmentation applied: random flips, color jitter for generalization

### 2. Model Training â€” Two Phase Strategy
- **Phase 1 (Epochs 1â€“4):** Backbone frozen, only MLP head trains â€” prevents catastrophic forgetting
- **Phase 2 (Epoch 5+):** Full fine-tuning with 10Ã— lower LR â€” adapts DINOv2 to desert domain
- Best model checkpoint saved automatically by validation IoU

### 3. Evaluation
- `test.py` runs inference on held-out `testImages/`
- Outputs per-image predictions, IoU scores, and colorized masks

### 4. Visualization & Failure Analysis
- Side-by-side RGB + predicted mask + ground truth (when available)
- Colorized masks using a distinct high-contrast palette per class
- Failure cases documented in `predictions/visualizations/`

---

## ğŸ› Troubleshooting

**Training is slow?**
- Reduce `--batch_size` to 2
- Reduce `--img_size` to 336 (still divisible by 14)
- Verify GPU is active: `python -c "import torch; print(torch.cuda.is_available())"`

**CUDA out of memory?**
- Set `--batch_size 2` and `--img_size 336`

**Segmentation masks not loading?**
- Ensure mask filenames exactly match RGB image filenames
- Verify mask pixel values match the class IDs in the table above

---

## ğŸ”® Future Enhancements

- [ ] Upgrade to DINOv2 ViT-B/14 for stronger features
- [ ] UPerNet / FPN decoder for multi-scale feature fusion
- [ ] Class-balanced sampling for rare classes (Logs, Flowers)
- [ ] Test-Time Augmentation (TTA) for inference boost without retraining
- [ ] Docker containerization for full reproducibility
- [ ] Multi-environment generalization beyond desert biomes

---

## ğŸ“ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Duality AI](https://www.duality.ai/) for the FalconCloud digital twin platform and dataset
- [Facebook Research](https://github.com/facebookresearch/dinov2) for DINOv2
- [PyTorch](https://pytorch.org/) for the deep learning framework
- [Anaconda](https://www.anaconda.com/) for environment management

---

## ğŸ”— Important Links

| Resource | Link |
|----------|------|
| Create a Falcon Account | [Sign Up](https://falcon.duality.ai/auth/sign-up?utm_source=hackathon&utm_medium=instructions&utm_campaign=GHR2) |
| Download Dataset | [Dataset Page](https://falcon.duality.ai/secure/documentation/hackathon-segmentation-desert?utm_source=hackathon&utm_medium=instructions&utm_campaign=GHR2) |
| Discord Community | [Join Server](https://discord.com/invite/dualityfalcommunity) |

---

## ğŸ† Judging Criteria

| Criteria | Points |
|----------|--------|
| IoU Score â€” pixel classification accuracy | 80 pts |
| Structured Findings & Detailed Reporting | 20 pts |
| **Total** | **100 pts** |

---

## ğŸ“ˆ Project Status

**Team**: Kairo
**Current Version**: 1.0.0
**Status**: Active Development
**Last Updated**: February 2026

---

<div align="center">

### Built by Team Kairo for the Duality AI Offroad Autonomy Segmentation Hackathon

Made with â¤ï¸ by Aditya Raj Â· Harsh Pal Â· Akshita Singh Â· Fuzailur Rahman

[Duality AI](https://www.duality.ai/) Â· [Falcon Platform](https://falcon.duality.ai/)

</div>
